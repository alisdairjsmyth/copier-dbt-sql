name: PR

on:
  pull_request:
    types: [edited, opened, reopened, synchronize]
    branches: [master]

# Default to read-only for all jobs; raise per job only if needed
permissions: read-all

concurrency:
  group: pr-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  check-title:
    runs-on: ubuntu-latest

    name: Check PR title

    steps:
      - uses: [[ gha__actions__checkout ]]
        with:
          ref: ${{ github.head_ref}}

      - name: Set up uv
        uses: [[ gha__astral_sh__setup_uv ]]

      - name: Check PR title
        run: |
          uvx --from=commitizen cz check --message "${{ github.event.pull_request.title }}"

  check-dbt-autofix:
    # Add permissions to allow posting PR comments
    permissions:
      contents: read
      checks: write
      pull-requests: write
      issues: write

    name: Check for deprecated dbt configuration
    runs-on: ubuntu-latest
    env:
      DBT_AUTOFIX_JSON: ./autofix.jsonl

    steps:
      - name: Checkout with full history
        uses: [[ gha__actions__checkout ]]
        with:
          ref: ${{ github.head_ref }}
          fetch-depth: 0

      - name: Set up Python
        uses: [[ gha__actions__setup_python ]]
        with:
          python-version-file: "pyproject.toml"

      - name: Install uv
        uses: [[ gha__astral_sh__setup_uv ]]

      # Capture JSONL output to a file (do not fail workflow if command fails)
      - name: Run dbt-autofix (JSONL)
        id: run_autofix
        shell: bash
        run: |
          set -o pipefail
          uvx dbt-autofix deprecations --dry-run --json | tee "$DBT_AUTOFIX_JSON" || true
          echo "dbt-autofix completed; output captured in $DBT_AUTOFIX_JSON (workflow will not fail)."

      # Build a Markdown report from the JSONL
      - name: Build PR comment body
        id: build_comment
        uses: [[ gha__actions__github_script ]]
        with:
          result-encoding: string
          script: |
            const fs = require('fs');
            const marker = '<!-- dbt-autofix-report -->';
            const title = '### dbt‑autofix: Deprecated dbt configuration';
            const path = process.env.DBT_AUTOFIX_JSON;
            const workspace = (process.env.GITHUB_WORKSPACE || '').replace(/\\/g, '/'); // normalize
            // Make a clean, repo-relative display path
            function toDisplayPath(p) {
              if (!p || typeof p !== 'string') return 'unknown';
              // Normalize slashes to forward
              let norm = p.replace(/\\/g, '/');
              // If the file path has duplicated workspace (rare), collapse it
              // e.g., /home/runner/work/repo/repo/repo/file -> first occurrence wins
              if (workspace && norm.startsWith(workspace)) {
                norm = norm.slice(workspace.length);
                if (norm.startsWith('/')) norm = norm.slice(1);
                return norm || 'unknown';
              }
              // Try relative from workspace using Node path (handles different roots)
              try {
                const rel = path.posix.relative(workspace || '/', norm);
                if (rel && !rel.startsWith('..')) return rel;
              } catch (_) { /* ignore */ }
              // Fallback to basename if we can't compute a nice relative path
              try {
                const base = path.posix.basename(norm);
                return base || norm;
              } catch (_) {
                return norm;
              }
            }
            let md = `${marker}\n${title}\n`;
            if (!fs.existsSync(path)) {
              return md + '\n_No output generated (no output file found)._';
            }
            const raw = fs.readFileSync(path, 'utf8').trim();
            if (!raw) {
              return md + '\n_No output generated (empty file)._';
            }
            const lines = raw.split('\n').filter(Boolean);
            // Parse lines
            const dryRuns = [];   // each corresponds to one file scan with refactors
            let sawComplete = false;
            for (const line of lines) {
              try {
                const obj = JSON.parse(line);
                if (obj && obj.mode === 'complete') {
                  sawComplete = true;
                } else if (obj && obj.mode === 'dry_run') {
                  dryRuns.push(obj);
                }
              } catch (e) {
                // ignore non‑JSON lines
              }
            }
            // Aggregate all refactors by file_path
            const byFile = {};
            let total = 0;
            for (const run of dryRuns) {
              const file = run.file_path || 'unknown';
              const refactors = Array.isArray(run.refactors) ? run.refactors : [];
              if (!byFile[file]) byFile[file] = [];
              byFile[file].push(...refactors);
              total += refactors.length;
            }
            if (sawComplete && total === 0) {
              md += '\n✅ No deprecations detected.\n';
            } else if (total > 0) {
              md += `\n⚠️ Found **${total}** deprecation${total === 1 ? '' : 's'} across **${Object.keys(byFile).length}** file${Object.keys(byFile).length === 1 ? '' : 's'} (dry run).\n\n`;
              for (const [file, refactors] of Object.entries(byFile)) {
                md += `**${toDisplayPath(file)}**\n`;
                for (const r of refactors) {
                  const dep = r.deprecation || 'Deprecation';
                  // Replace single quotes with backticks in the log text
                  const log =
                    typeof r.log === 'string'
                      ? r.log.replace(/'/g, '`')
                      : '';
                  md += `- \`${dep}\`${log ? ` — ${log}` : ''}\n`;
                }
                md += '\n';
              }
              // Raw JSONL for traceability
              md += `<details><summary>Raw JSONL output</summary>\n\n\`\`\`json\n${raw}\n\`\`\`\n</details>\n`;
            } else {
              // No 'complete' sentinel and no refactors => ambiguous/no output
              md += '\n_Output did not include expected markers. See raw output below._\n';
              md += `<details><summary>Raw JSONL output</summary>\n\n\`\`\`\n${raw}\n\`\`\`\n</details>\n`;
            }
            return md;

      # Add the same report to the Action run summary
      - name: Add report to job summary
        run: |
          cat <<'EOF' >> "$GITHUB_STEP_SUMMARY"
          ${{ steps.build_comment.outputs.result }}
          EOF

      # Create or update a PR comment
      - name: Comment on PR with dbt‑autofix results
        uses: [[ gha__actions__github_script ]]
        env:
          BODY: ${{ steps.build_comment.outputs.result }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const body = process.env.BODY;
            const marker = '<!-- dbt-autofix-report -->';
            const { owner, repo } = context.repo;
            const issue_number = context.payload.pull_request.number;
            const { data: comments } = await github.rest.issues.listComments({
              owner, repo, issue_number, per_page: 100
            });
            const existing = comments.find(c => c.body && c.body.includes(marker));
            if (existing) {
              await github.rest.issues.updateComment({
                owner, repo, comment_id: existing.id, body
              });
            } else {
              await github.rest.issues.createComment({
                owner, repo, issue_number, body
              });
            }

  sqlfluff-lint:
    # Needs to read code and publish GitHub Checks annotations
    permissions:
      contents: read
      checks: write
      pull-requests: write

    name: Lint dbt project
    runs-on: ubuntu-latest

    env:
      ANNOTATIONS_FILE: ./annotations.json
      DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
      DATABRICKS_HTTP_PATH: ${{ vars.DATABRICKS_HTTP_PATH }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      DBT_PROFILES_DIR: ./ci_cd

    steps:
      - name: Checkoput with full history
        uses: [[ gha__actions__checkout ]]
        with:
          ref: ${{ github.head_ref}}
          fetch-depth: 0

      - name: Fetch base branch
        run: |
          git fetch origin "${{ github.base_ref }}"

      - name: Set up Python
        uses: [[ gha__actions__setup_python ]]
        with:
          python-version-file: "pyproject.toml"

      - name: Install uv
        uses: [[ gha__astral_sh__setup_uv ]]

      - name: Install dependencies
        run: |
          uv sync --locked --all-extras --dev

      - name: Check dbt connection
        run: |
          source .venv/bin/activate
          dbt debug
          dbt deps

      - name: Get changed files
        id: get_file_changes
        shell: bash
        run: |
          set -euo pipefail

          # Show context
          echo "Base ref: ${{ github.base_ref }}"
          echo "Head ref: ${{ github.head_ref }}"

          # List files changed between base...HEAD in the PR
          mapfile -t FILES_ALL < <(git diff --name-only --diff-filter=ACMR "origin/${{ github.base_ref }}"...HEAD)

          # Join as space-separated
          FILES_SPC=$(printf './%s ' "${FILES_ALL[@]}")

          # Export outputs for subsequent steps
          {
            echo "files_modified=${FILES_SPC% }"
            echo "files_all<<EOF"
            printf '%s\n' "${FILES_ALL[@]}"
            echo "EOF"
          } >> "$GITHUB_OUTPUT"

      - name: Get new and changed .sql files in ./src/models to lint
        id: get_files_to_lint
        shell: bash
        run: |
          set -euo pipefail
          files=$(
            printf '%s\n' ${{ steps.get_file_changes.outputs.files_modified }} \
              | tr -s ' ' '\n' \
              | grep -E '^(\./)?src/models/.*\.sql$' \
              | sort -u \
              || true
          )
          {
            echo "lintees<<EOF"
            printf '%s\n' "$files"
            echo "EOF"
          } >> "$GITHUB_OUTPUT"

      - name: Lint dbt models
        if: steps.get_files_to_lint.outputs.lintees != ''
        shell: bash
        run: |
          source .venv/bin/activate
          # Read multiline output and pass to sqlfluff via xargs
          echo "${{ steps.get_files_to_lint.outputs.lintees }}" \
            [% raw %]| grep -v '^[[:space:]]*$' \[% endraw %]
            | xargs -r sqlfluff lint \
              --templater dbt \
              --format github-annotation \
              --annotation-level failure \
              --nofail \
              --write-output "$ANNOTATIONS_FILE" || true

      - name: Check for annotations
        id: check_annotations
        if: steps.get_files_to_lint.outputs.lintees != ''
        shell: bash
        run: |
          set -euo pipefail
          has_annotations=false
          if [ -s "$ANNOTATIONS_FILE" ] && (
            command -v jq >/dev/null 2>&1 && jq -e 'type=="array" and length>0' "$ANNOTATIONS_FILE" >/dev/null 2>&1
          ); then
            has_annotations=true
          fi
          echo "has_annotations=${has_annotations}" >> "$GITHUB_OUTPUT"

      - name: Show annotations file contents
        shell: bash
        if: steps.get_files_to_lint.outputs.lintees != ''
        run: |
          echo "----- Debug: Checking $ANNOTATIONS_FILE -----"

          if [ ! -f "$ANNOTATIONS_FILE" ]; then
            echo "$ANNOTATIONS_FILE does NOT exist."
            exit 0
          fi

          echo "----- Raw file contents -----"
          cat "$ANNOTATIONS_FILE" | head -c 1000000 || true

          echo "----- Attempt JSON pretty-print -----"
          jq . "$ANNOTATIONS_FILE" || echo "(File is not valid JSON)"

      - name: Annotate
        if: steps.check_annotations.outputs.has_annotations == 'true'
        uses: [[ gha__yuzutech__annotations_action ]]
        with:
          repo-token: "${{ secrets.GITHUB_TOKEN }}"
          title: "SQLFluff Lint"
          input: "${{ env.ANNOTATIONS_FILE }}"
          ignore-missing-file: false

  check-sqlfmt:
    permissions:
      contents: read

    name: Check SQL formatting
    runs-on: ubuntu-latest

    steps:
      - name: Checkout with full history
        uses: [[ gha__actions__checkout ]]
        with:
          ref: ${{ github.head_ref }}
          fetch-depth: 0

      - name: Set up Python
        uses: [[ gha__actions__setup_python ]]
        with:
          python-version-file: "pyproject.toml"

      - name: Install uv
        uses: [[ gha__astral_sh__setup_uv ]]

      - name: Install dependencies
        run: |
          uv sync --locked --all-extras --dev

      - name: Get Changed Files
        id: changed-files
        uses: tj-actions/changed-files@v46

      - name: Run sqlfmt
        shell: bash
        run: |
          source .venv/bin/activate
          sqlfmt --diff ${{ steps.changed-files.outputs.all_changed_files }}
